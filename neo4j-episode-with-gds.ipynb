{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597f9cad",
   "metadata": {},
   "source": [
    "# Neo4j Aura Professional Trial Instance : Episode with Graph Data Science (GDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc58257",
   "metadata": {},
   "source": [
    "## Use APIKey Credentials generated to work with GDS calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11601a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded:\n",
      "  NEO4J_URI: neo4j+s://2236ba22.databases.neo4j.io\n",
      "  NEO4J_USERNAME: neo4j\n",
      "  NEO4J_DATABASE: neo4j\n",
      "  AURA_INSTANCEID: 2236ba22\n",
      "  AURA_INSTANCENAME: Instance02\n",
      "  CLIENT_ID: Z62ZOi3OpauFjwLDGa9eQngrTLV78BCJ\n",
      "  CLIENT_NAME: MentalModelAuraPAPIKey\n",
      "  CLIENT_SECRET: ****************************************************************\n",
      "  OPENAI_API_KEY: ********************************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from neo4j_viz.neo4j import from_neo4j\n",
    "\n",
    "#get env setup for Free Trial\n",
    "#load_dotenv('.env', override=True)\n",
    "\n",
    "#get env set up for PROFESSIONAL free trial with data science graph algorithms enabled\n",
    "load_dotenv('podcast-gds.env', override=True)\n",
    "\n",
    "# Neo4j connection variables\n",
    "if not os.environ.get('NEO4J_URI'):\n",
    "    os.environ['NEO4J_URI'] = getpass.getpass('NEO4J_URI:\\n')\n",
    "if not os.environ.get('NEO4J_USERNAME'):\n",
    "    os.environ['NEO4J_USERNAME'] = getpass.getpass('NEO4J_USERNAME:\\n')\n",
    "if not os.environ.get('NEO4J_PASSWORD'):\n",
    "    os.environ['NEO4J_PASSWORD'] = getpass.getpass('NEO4J_PASSWORD:\\n')\n",
    "\n",
    "# Client credentials for API access\n",
    "if not os.environ.get('CLIENT_SECRET'):\n",
    "    os.environ['CLIENT_SECRET'] = getpass.getpass('CLIENT_SECRET:\\n')\n",
    "if not os.environ.get('CLIENT_ID'):\n",
    "    os.environ['CLIENT_ID'] = getpass.getpass('CLIENT_ID:\\n')\n",
    "if not os.environ.get('CLIENT_NAME'):\n",
    "    os.environ['CLIENT_NAME'] = getpass.getpass('CLIENT_NAME:\\n')\n",
    "\n",
    "# Assign environment variables\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')\n",
    "AURA_INSTANCEID = os.getenv('AURA_INSTANCEID')\n",
    "AURA_INSTANCENAME = os.getenv('AURA_INSTANCENAME')\n",
    "\n",
    "# Client credentials\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_NAME = os.getenv('CLIENT_NAME')\n",
    "\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Print loaded credentials (without sensitive values)\n",
    "print(\"‚úÖ Environment variables loaded:\")\n",
    "print(f\"  NEO4J_URI: {NEO4J_URI}\")\n",
    "print(f\"  NEO4J_USERNAME: {NEO4J_USERNAME}\")\n",
    "print(f\"  NEO4J_DATABASE: {NEO4J_DATABASE}\")\n",
    "print(f\"  AURA_INSTANCEID: {AURA_INSTANCEID}\")\n",
    "print(f\"  AURA_INSTANCENAME: {AURA_INSTANCENAME}\")\n",
    "print(f\"  CLIENT_ID: {CLIENT_ID}\")\n",
    "print(f\"  CLIENT_NAME: {CLIENT_NAME}\")\n",
    "print(f\"  CLIENT_SECRET: {'*' * len(CLIENT_SECRET) if CLIENT_SECRET else 'Not set'}\")\n",
    "print(f\"  OPENAI_API_KEY: {'*' * len(OPENAI_API_KEY) if OPENAI_API_KEY else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5232fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, parameters=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters)\n",
    "        # Collect results as a list\n",
    "        records = [record.data() for record in result]\n",
    "        # Print the records\n",
    "        for record in records:\n",
    "            print(record)\n",
    "        return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to run and display Cypher query results\n",
    "def run_query_v(query, parameters=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters)\n",
    "        VG = from_neo4j(result)\n",
    "        return VG\n",
    "\n",
    "# Run the query; check the connection\n",
    "query = \"MATCH p=()-[]-() limit 20 RETURN p\"\n",
    "vis = run_query_v(query)\n",
    "vis.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da9afd",
   "metadata": {},
   "source": [
    "#### Check GDS Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    # Test 1: Basic GDS version check\n",
    "    try:\n",
    "        result = session.run(\"RETURN gds.version()\")\n",
    "        # Collect results as a list\n",
    "        records = [record.data() for record in result]\n",
    "        #version = result.single()['version']\n",
    "        version = records[0]['gds.version()']\n",
    "        print(f\"‚úÖ GDS Version: {version}\")\n",
    "        print(\"‚úÖ GDS is available and working!\")\n",
    "\n",
    "        # Print the records\n",
    "        for record in records:\n",
    "            print(record)\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GDS Version Check Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def project_graph(tx, source_node_label=\"Episode\", target_node_label=\"Topic\", relationship_label=\"HAS_TOPIC\", graph_name=\"topicGraph\"):\n",
    "    # Project Episode-Topic graph using standard GDS procedures\n",
    "    try:\n",
    "        # First, try the standard GDS procedure (works on most Neo4j instances)\n",
    "        result = tx.run(\n",
    "            \"\"\"\n",
    "            CALL gds.graph.project(\n",
    "                $graph_name,\n",
    "                [$source_node_label,$target_node_label],\n",
    "                [$relationship_label]\n",
    "            )\n",
    "            YIELD graphName, nodeCount, relationshipCount\n",
    "            RETURN graphName, nodeCount, relationshipCount\n",
    "            \"\"\",\n",
    "            graph_name=graph_name, source_node_label=source_node_label, target_node_label=target_node_label, relationship_label=relationship_label\n",
    "        )\n",
    "        return result.single()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored GDS functions with shared exception handling\n",
    "def handle_gds_fallback(tx, operation_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Handle GDS fallback when procedures are not found\n",
    "    Args:\n",
    "        tx: Neo4j transaction\n",
    "        operation_type: 'project' or 'similarity'\n",
    "        **kwargs: Additional parameters for the operation\n",
    "    \"\"\"\n",
    "    if operation_type == 'project':\n",
    "        print(\"‚ö†Ô∏è  GDS not available, creating simple graph projection...\")\n",
    "        result = tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (e:Episode)-[r:HAS_TOPIC]->(t:Topic)\n",
    "            WITH e, t, r\n",
    "            RETURN count(e) as episodeCount, count(t) as topicCount, count(r) as relationshipCount\n",
    "            \"\"\"\n",
    "        )\n",
    "        return result.single()\n",
    "    \n",
    "    elif operation_type == 'similarity':\n",
    "        print(\"‚ö†Ô∏è  GDS not available, using Jaccard similarity on shared topics...\")\n",
    "        episode_number = kwargs.get('episode_number')\n",
    "        top_n = kwargs.get('top_n', 5)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Episode {number: $episode_number})-[:HAS_TOPIC]->(t:Topic)<-[:HAS_TOPIC]-(e2:Episode)\n",
    "        WHERE e1 <> e2\n",
    "        WITH e1, e2, count(t) as shared_topics\n",
    "        MATCH (e1)-[:HAS_TOPIC]->(t1:Topic)\n",
    "        MATCH (e2)-[:HAS_TOPIC]->(t2:Topic)\n",
    "        WITH e1, e2, shared_topics, count(DISTINCT t1) as topics1, count(DISTINCT t2) as topics2\n",
    "        WITH e1, e2, shared_topics, topics1, topics2, \n",
    "             shared_topics * 1.0 / (topics1 + topics2 - shared_topics) as similarity\n",
    "        RETURN e2.number AS similar_episode_number, e2.name AS similar_episode_name, similarity\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT $top_n\n",
    "        \"\"\"\n",
    "        result = tx.run(query, episode_number=episode_number, top_n=top_n)\n",
    "        return [record.data() for record in result]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown operation type: {operation_type}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_graph_refactored(tx, graph_name=\"topicGraph\"):\n",
    "    \"\"\"Project Episode-Topic graph using standard GDS procedures with fallback\"\"\"\n",
    "    try:\n",
    "        # First, try the standard GDS procedure (works on most Neo4j instances)\n",
    "        result = tx.run(\n",
    "            \"\"\"\n",
    "            CALL gds.graph.project(\n",
    "                $graph_name,\n",
    "                ['Episode','Topic'],\n",
    "                ['HAS_TOPIC'],\n",
    "                {memory: 'HEAP:1G'}\n",
    "            )\n",
    "            YIELD graphName, nodeCount, relationshipCount\n",
    "            RETURN graphName, nodeCount, relationshipCount\n",
    "            \"\"\",\n",
    "            graph_name=graph_name\n",
    "        )\n",
    "        return result.single()\n",
    "    except Exception as e:\n",
    "        if \"ProcedureNotFound\" in str(e):\n",
    "            return handle_gds_fallback(tx, 'project')\n",
    "        else:\n",
    "            raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c84833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_episode_similarity(tx, graph_name=\"topicGraph\", episode_number=None, top_n=5):\n",
    "    # Compute node similarity using GDS nodeSimilarity\n",
    "    query = f\"\"\"\n",
    "    CALL gds.nodeSimilarity.stream('{graph_name}')\n",
    "    YIELD node1, node2, similarity\n",
    "    WITH gds.util.asNode(node1) AS ep1, gds.util.asNode(node2) AS ep2, similarity\n",
    "    WHERE ep1.number = $episode_number\n",
    "    RETURN ep2.number AS similar_episode_number, ep2.name AS similar_episode_name, similarity\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_n\n",
    "    \"\"\"\n",
    "    result = tx.run(query, episode_number=episode_number, top_n=top_n)\n",
    "    return [record.data() for record in result]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_episode_similarity_refactored(tx, graph_name=\"topicGraph\", episode_number=None, top_n=5):\n",
    "    \"\"\"Compute episode similarity using GDS or fallback method\"\"\"\n",
    "    try:\n",
    "        # Try GDS nodeSimilarity first\n",
    "        query = f\"\"\"\n",
    "        CALL gds.nodeSimilarity.stream('{graph_name}')\n",
    "        YIELD node1, node2, similarity\n",
    "        WITH gds.util.asNode(node1) AS ep1, gds.util.asNode(node2) AS ep2, similarity\n",
    "        WHERE ep1.number = $episode_number\n",
    "        RETURN ep2.number AS similar_episode_number, ep2.name AS similar_episode_name, similarity\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT $top_n\n",
    "        \"\"\"\n",
    "        result = tx.run(query, episode_number=episode_number, top_n=top_n)\n",
    "        return [record.data() for record in result]\n",
    "    except Exception as e:\n",
    "        if \"ProcedureNotFound\" in str(e):\n",
    "            return handle_gds_fallback(tx, 'similarity', episode_number=episode_number, top_n=top_n)\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_graph(tx, graph_name=\"topicGraph\"):\n",
    "    tx.run(f\"CALL gds.graph.drop('{graph_name}') YIELD graphName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11946557",
   "metadata": {},
   "source": [
    "#### Find Similar Episodes based on Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Usage ---\n",
    "episode_number_to_query = 473  # Example episode\n",
    "with driver.session() as session:\n",
    "    print(\"Projecting graph...\")\n",
    "    info = session.execute_write(project_graph, \"Episode\", \"Topic\", \"HAS_TOPIC\", \"topicGraph\")\n",
    "    print(\"Graph projected:\", info)\n",
    "\n",
    "    print(f\"\\nTop similar episodes for Episode #{episode_number_to_query}:\")\n",
    "    similar_eps = session.execute_read(compute_episode_similarity, \"topicGraph\", episode_number_to_query, 3)\n",
    "    for ep in similar_eps:\n",
    "        print(ep)\n",
    "\n",
    "    print(\"\\nCleaning up in-memory graph...\")\n",
    "    session.execute_write(drop_graph, \"topicGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad9328",
   "metadata": {},
   "source": [
    "#### Finding Similar Episodes based on Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Usage ---\n",
    "episode_number_to_query = 473  # Example episode\n",
    "with driver.session() as session:\n",
    "    print(\"Projecting graph...\")\n",
    "    info = session.execute_write(project_graph, \"Episode\", \"Technology\", \"COVERS_TECHNOLOGY\", \"technologyGraph\")\n",
    "    print(\"Graph projected:\", info)\n",
    "\n",
    "    print(f\"\\nTop similar episodes for Episode #{episode_number_to_query}:\")\n",
    "    similar_eps = session.execute_read(compute_episode_similarity, \"technologyGraph\", episode_number_to_query, 3)\n",
    "    for ep in similar_eps:\n",
    "        print(ep)\n",
    "\n",
    "    print(\"\\nCleaning up in-memory graph...\")\n",
    "    session.execute_write(drop_graph, \"technologyGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11c566",
   "metadata": {},
   "source": [
    "#### Manually delete a graph projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f15ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.graph.drop('episode-tech-projection');\n",
    "\"\"\"\n",
    "try:\n",
    "    results = run_query(query)\n",
    "except Exception:\n",
    "    print(\"Ignoring error - projection does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b562955",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978b060",
   "metadata": {},
   "source": [
    "## GDS Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457b9a4",
   "metadata": {},
   "source": [
    "#### List available GDS graph projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47815825",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "        \n",
    "    # Test 2: Check for specific GDS procedures\n",
    "    try:\n",
    "        result = session.run(\"CALL gds.graph.list()\")\n",
    "        graphs = [record['graphName'] for record in result]\n",
    "        print(f\"üìä Current GDS Graphs: {len(graphs)}\")\n",
    "        if graphs:\n",
    "            for graph in graphs:\n",
    "                print(f\"  - {graph}\")\n",
    "        else:\n",
    "            print(\"  (No graphs currently loaded)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GDS List Check Failed: {e}\")\n",
    "        \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef88d0f",
   "metadata": {},
   "source": [
    "#### List details of available Graph Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "        \n",
    "    # Test 2: Check for specific GDS procedures\n",
    "    try:\n",
    "        result = session.run(\"CALL gds.graph.list() YIELD graphName, nodeCount, relationshipCount, schema\") \n",
    "        print(f\"üìä Graph Projections:\")\n",
    "        for record in result:\n",
    "            print(f\"  - {record['graphName']}:\")\n",
    "            print(f\"    Nodes: {record['nodeCount']}\")\n",
    "            print(f\"    Relationships: {record['relationshipCount']}\")\n",
    "            print(f\"    Schema: {record['schema']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GDS List Check Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303cadf",
   "metadata": {},
   "source": [
    "#### Test Refacrtored Graph projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the refactored functions\n",
    "print(\"üß™ Testing Refactored GDS Functions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "episode_number_to_query = 473  # Example episode\n",
    "\n",
    "with driver.session() as session:\n",
    "    print(\"1. Testing graph projection...\")\n",
    "    try:\n",
    "        info = session.execute_write(project_graph_refactored, \"topicGraph\")\n",
    "        print(f\"‚úÖ Graph projected: {info}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Graph projection failed: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed96b67",
   "metadata": {},
   "source": [
    "#### Test Refactored Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d89f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    print(f\"\\n2. Testing similarity computation for Episode #{episode_number_to_query}...\")\n",
    "    try:\n",
    "        similar_eps = session.execute_read(\n",
    "            compute_episode_similarity_refactored, \n",
    "            \"topicGraph\", \n",
    "            episode_number_to_query, \n",
    "            3\n",
    "        )\n",
    "        print(f\"‚úÖ Found {len(similar_eps)} similar episodes:\")\n",
    "        for ep in similar_eps:\n",
    "            print(f\"  - Episode {ep['similar_episode_number']}: {ep['similar_episode_name']} (similarity: {ep['similarity']:.3f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Similarity computation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edd3d8",
   "metadata": {},
   "source": [
    "#### Test GDS Fallback method directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    print(f\"\\n3. Testing fallback method directly...\")\n",
    "    try:\n",
    "        # Test the fallback method directly\n",
    "        fallback_result = session.execute_read(\n",
    "            lambda tx: handle_gds_fallback(tx, 'similarity', episode_number=episode_number_to_query, top_n=2)\n",
    "        )\n",
    "        print(f\"‚úÖ Fallback method works: Found {len(fallback_result)} similar episodes\")\n",
    "        for ep in fallback_result:\n",
    "            print(f\"  - Episode {ep['similar_episode_number']}: {ep['similar_episode_name']} (similarity: {ep['similarity']:.3f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback method failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d064f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Vector Indexes (no creation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85d765",
   "metadata": {},
   "source": [
    "## Create and work with Vector Index on Chunk node and KNN via GDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791cb9a",
   "metadata": {},
   "source": [
    "#### Show Vector Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75391ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Showing vector indexes...\")\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"SHOW VECTOR INDEXES\")\n",
    "    indexes = [record.data() for record in result]\n",
    "\n",
    "    if not indexes:\n",
    "        print(\"‚ö†Ô∏è  No vector indexes found.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(indexes)} vector index(es):\\n\")\n",
    "        for idx in indexes:\n",
    "            print(f\"- Name: {idx.get('name', 'N/A')}\")\n",
    "            print(f\"  Type: {idx.get('type', 'N/A')}\")\n",
    "            print(f\"  State: {idx.get('state', 'N/A')}\")\n",
    "            if 'properties' in idx:\n",
    "                print(f\"  Properties: {idx.get('properties', [])}\")\n",
    "            print()\n",
    "\n",
    "    has_chunk_index = any(i.get('name') == 'chunkIndex' for i in indexes)\n",
    "    print(f\"üîé chunkIndex present: {'Yes' if has_chunk_index else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826741e",
   "metadata": {},
   "source": [
    "#### Create question embedding and Define method to anwer user question using Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57654272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector search functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Vector Search with Question Embedding and Episode Retrieval\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def create_question_embedding(question):\n",
    "    \"\"\"\n",
    "    Create an embedding for a user's question using OpenAI's text-embedding-3-small model.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question text\n",
    "        \n",
    "    Returns:\n",
    "        list: A 1536-dimensional vector embedding of the question\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=question\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def search_episodes_by_question(question, k=5):\n",
    "    \"\"\"\n",
    "    Search for relevant episodes using vector similarity search on chunk embeddings.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question\n",
    "        k (int): Number of nearest neighbor chunks to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing EpisodeTitle, ChunkContent, and SimilarityScore\n",
    "    \"\"\"\n",
    "    # Step 1: Create embedding for the question\n",
    "    question_embedding = create_question_embedding(question)\n",
    "    \n",
    "    # Step 2: Execute vector search query\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            // Step 1: Query the vector index ('chunkIndex') to find the most similar chunks.\n",
    "            // $questionEmbedding is the list of floats/integers representing the user's question.\n",
    "            // $k specifies the number of nearest neighboring chunks to retrieve.\n",
    "            CALL db.index.vector.queryNodes(\n",
    "                'chunkIndex',\n",
    "                $k,\n",
    "                $questionEmbedding\n",
    "            )\n",
    "            YIELD node AS chunk, score\n",
    "\n",
    "            // Step 2: Match the relationship to find the parent Episode.\n",
    "            // We use the inverse direction of the BELONGS_TO relationship \n",
    "            // to go from the retrieved Chunk node back to the Episode node.\n",
    "            MATCH (episode:Episode)<-[:BELONGS_TO_EPISODE]-(chunk)\n",
    "\n",
    "            // Step 3: Return the results, ordered by similarity score.\n",
    "            RETURN\n",
    "                episode.name AS EpisodeTitle,\n",
    "                episode.number AS EpisodeNumber,\n",
    "                // Return properties of the matching chunk (e.g., its content)\n",
    "                chunk.text AS ChunkContent, \n",
    "                score AS SimilarityScore\n",
    "            ORDER BY\n",
    "                SimilarityScore DESC\n",
    "        \"\"\", questionEmbedding=question_embedding, k=k)\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for record in result:\n",
    "            results.append({\n",
    "                'EpisodeTitle': record['EpisodeTitle'],\n",
    "                'EpisodeNumber': record['EpisodeNumber'],\n",
    "                'ChunkContent': record['ChunkContent'],\n",
    "                'SimilarityScore': record['SimilarityScore']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Vector search functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df4754",
   "metadata": {},
   "source": [
    "### Define Hybrid search to respond to user question (Vector search followed by KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcbda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GDS-enhanced vector search function loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# GDS-Enhanced Vector Search with KNN Relationship Traversal\n",
    "\n",
    "def search_episodes_gds_by_question(question, k=5, limit=10):\n",
    "    \"\"\"\n",
    "    Extended search that combines vector search with GDS KNN relationships.\n",
    "    \n",
    "    This method:\n",
    "    1. Performs vector search to find seed episodes (like search_episodes_by_question)\n",
    "    2. Follows pre-calculated SEMANTICALLY_SIMILAR_KNN relationships from seed episodes\n",
    "    3. Combines and ranks results using both index scores and KNN similarity scores\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question\n",
    "        k (int): Number of nearest neighbor chunks to retrieve for initial search (default: 5)\n",
    "        limit (int): Total number of results to return (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing:\n",
    "            - SeedEpisode: Name of the episode found via vector search\n",
    "            - SeedEpisode_IndexScore: Similarity score from vector index\n",
    "            - SimilarEpisode: Name of the episode found via KNN relationship\n",
    "            - KNN_Similarity_Score: Pre-calculated KNN similarity score\n",
    "    \"\"\"\n",
    "    # Step 1: Create embedding for the question\n",
    "    question_embedding = create_question_embedding(question)\n",
    "    \n",
    "    # Step 2: Execute combined vector search + GDS KNN query\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            // Step 1-2: Query the vector index and find seed episodes\n",
    "            CALL db.index.vector.queryNodes(\n",
    "                'chunkIndex',\n",
    "                $k,\n",
    "                $questionEmbedding\n",
    "            )\n",
    "            YIELD node AS chunk, score AS indexScore\n",
    "\n",
    "            // Match the relationship to find the parent Episode (seed episode)\n",
    "            MATCH (seedEpisode:Episode)<-[:BELONGS_TO_EPISODE]-(chunk)\n",
    "\n",
    "            // Step 3: Follow the pre-calculated KNN relationships from the seed episodes\n",
    "            OPTIONAL MATCH (seedEpisode)-[r:SEMANTICALLY_SIMILAR_KNN]->(similarEpisode:Episode)\n",
    "\n",
    "            // Step 4: Combine and rank the results\n",
    "            RETURN DISTINCT // Use DISTINCT to avoid duplicates if multiple seeds point to the same episode\n",
    "                seedEpisode.name AS SeedEpisode,\n",
    "                seedEpisode.number AS SeedEpisodeNumber,\n",
    "                indexScore AS SeedEpisode_IndexScore,\n",
    "                similarEpisode.name AS SimilarEpisode,\n",
    "                similarEpisode.number AS SimilarEpisodeNumber,\n",
    "                r.knn_score AS KNN_Similarity_Score\n",
    "            ORDER BY \n",
    "                SeedEpisode_IndexScore DESC, // Prioritize results from a stronger index match\n",
    "                KNN_Similarity_Score DESC // Use KNN score as a secondary rank\n",
    "            LIMIT $limit // Return the top N overall results\n",
    "        \"\"\", questionEmbedding=question_embedding, k=k, limit=limit)\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for record in result:\n",
    "            results.append({\n",
    "                'SeedEpisode': record['SeedEpisode'],\n",
    "                'SeedEpisodeNumber': record['SeedEpisodeNumber'],\n",
    "                'SeedEpisode_IndexScore': record['SeedEpisode_IndexScore'],\n",
    "                'SimilarEpisode': record.get('SimilarEpisode'),  # May be None if no KNN relationship\n",
    "                'SimilarEpisodeNumber': record.get('SimilarEpisodeNumber'),\n",
    "                'KNN_Similarity_Score': record.get('KNN_Similarity_Score')\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ GDS-enhanced vector search function loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e2338",
   "metadata": {},
   "source": [
    "## Test Vector Index on Chunk node with hybrid method (Vector Index + KNN search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7505769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç GDS-Enhanced Search for: 'What is Apache Iceberg and how does it work?'\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Found 10 results (combining vector search + KNN relationships):\n",
      "\n",
      "1. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.8223\n",
      "   ‚Üí Similar Episode (via KNN): #473 - Delayed View Semantics In Incremental Data Processing\n",
      "   KNN Similarity Score: 0.8930\n",
      "\n",
      "2. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.8223\n",
      "   ‚Üí Similar Episode (via KNN): #480 - Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   KNN Similarity Score: 0.8833\n",
      "\n",
      "3. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.8177\n",
      "   ‚Üí Similar Episode (via KNN): #473 - Delayed View Semantics In Incremental Data Processing\n",
      "   KNN Similarity Score: 0.8930\n",
      "\n",
      "4. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.8177\n",
      "   ‚Üí Similar Episode (via KNN): #480 - Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   KNN Similarity Score: 0.8833\n",
      "\n",
      "5. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7868\n",
      "   ‚Üí Similar Episode (via KNN): #473 - Delayed View Semantics In Incremental Data Processing\n",
      "   KNN Similarity Score: 0.8930\n",
      "\n",
      "6. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7868\n",
      "   ‚Üí Similar Episode (via KNN): #480 - Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   KNN Similarity Score: 0.8833\n",
      "\n",
      "7. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7704\n",
      "   ‚Üí Similar Episode (via KNN): #473 - Delayed View Semantics In Incremental Data Processing\n",
      "   KNN Similarity Score: 0.8930\n",
      "\n",
      "8. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7704\n",
      "   ‚Üí Similar Episode (via KNN): #480 - Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   KNN Similarity Score: 0.8833\n",
      "\n",
      "9. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7703\n",
      "   ‚Üí Similar Episode (via KNN): #473 - Delayed View Semantics In Incremental Data Processing\n",
      "   KNN Similarity Score: 0.8930\n",
      "\n",
      "10. Seed Episode: #1654 - Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Vector Index Score: 0.7703\n",
      "   ‚Üí Similar Episode (via KNN): #480 - Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   KNN Similarity Score: 0.8833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Usage: GDS-Enhanced Vector Search with KNN Relationships\n",
    "\n",
    "# Example question\n",
    "user_question = \"What is Apache Iceberg and how does it work?\"  \n",
    "#user_question = \"What are the top 2 episodes that cover Data Lake concepts\"\n",
    "\n",
    "print(f\"üîç GDS-Enhanced Search for: '{user_question}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Perform the GDS-enhanced search\n",
    "results = search_episodes_gds_by_question(user_question, k=5, limit=10)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    print(f\"\\n‚úÖ Found {len(results)} results (combining vector search + KNN relationships):\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Seed Episode: #{result['SeedEpisodeNumber']} - {result['SeedEpisode']}\")\n",
    "        print(f\"   Vector Index Score: {result['SeedEpisode_IndexScore']:.4f}\")\n",
    "        \n",
    "        if result.get('SimilarEpisode'):\n",
    "            print(f\"   ‚Üí Similar Episode (via KNN): #{result['SimilarEpisodeNumber']} - {result['SimilarEpisode']}\")\n",
    "            print(f\"   KNN Similarity Score: {result['KNN_Similarity_Score']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   ‚Üí No KNN relationships found for this seed episode\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results found. Make sure:\")\n",
    "    print(\"  1. The 'chunkIndex' vector index exists\")\n",
    "    print(\"  2. Chunk nodes have embeddings\")\n",
    "    print(\"  3. SEMANTICALLY_SIMILAR_KNN relationships exist between Episode nodes\")\n",
    "    print(\"  4. KNN relationships have knn_score property\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53ed37",
   "metadata": {},
   "source": [
    "## Test Vector Index on Chunk node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5cdbdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for episodes related to: 'What are the top 2 episodes that cover Data Lake concepts'\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Found 5 relevant chunks:\n",
      "\n",
      "1. Episode #480: Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   Similarity Score: 0.7620\n",
      "   Chunk Content: Summary\n",
      "In this episode of the Data Engineering Podcast Hannes M√ºhleisen and Mark Raasveldt, the creators of DuckDB, share their work on Duck Lake, a new entrant in the open lakehouse ecosystem. They ...\n",
      "\n",
      "2. Episode #1654: Iceberg at Netflix and Beyond with Ryan Blue\n",
      "   Similarity Score: 0.7401\n",
      "   Chunk Content:  the ability to have a swappable storage layer. It's the storage layer that you can use underneath Spark. But you can also use it underneath Snowflake now, which is really crazy. When Snowflake starte...\n",
      "\n",
      "3. Episode #480: Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   Similarity Score: 0.7317\n",
      "   Chunk Content:  like, of course, I knew how   SQL worked, like, what a database was, but I'd never really thought about the concept of creating a database, like, the the back end. And I was like, that sounds super i...\n",
      "\n",
      "4. Episode #480: Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   Similarity Score: 0.7288\n",
      "   Chunk Content:  gravity sort of remaining besides   sort of between data centers. Right? I think that's that's still something that people underestimate generally. But we do we do have this capability of putting put...\n",
      "\n",
      "5. Episode #480: Simplifying Lakehouse Ecosystem With DuckLake\n",
      "   Similarity Score: 0.7279\n",
      "   Chunk Content:  on top of a on top of a lot that we have, but I think we're also improving all of these things constantly, which means that it's constantly getting better as well. So I think that's that's also reall...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Usage: Vector Search with Question Embedding\n",
    "\n",
    "# Example question\n",
    "#user_question_one = \"What is Apache Iceberg and how does it work?\" \n",
    "user_question = \"What are the top 2 episodes that cover Data Lake concepts\"\n",
    "\n",
    "\n",
    "print(f\"üîç Searching for episodes related to: '{user_question}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Perform the search\n",
    "results = search_episodes_by_question(user_question, k=5)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    print(f\"\\n‚úÖ Found {len(results)} relevant chunks:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Episode #{result['EpisodeNumber']}: {result['EpisodeTitle']}\")\n",
    "        print(f\"   Similarity Score: {result['SimilarityScore']:.4f}\")\n",
    "        print(f\"   Chunk Content: {result['ChunkContent'][:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results found. Make sure:\")\n",
    "    print(\"  1. The 'chunkIndex' vector index exists\")\n",
    "    print(\"  2. Chunk nodes have embeddings\")\n",
    "    print(\"  3. Chunks are connected to Episodes via BELONGS_TO relationship\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-employee-graph",
   "language": "python",
   "name": "neo4j-employee-graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
