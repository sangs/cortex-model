{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82db304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#get env setup\n",
    "load_dotenv('podcast-gds.env', override=True)\n",
    "\n",
    "if not os.environ.get('NEO4J_URI'):\n",
    "    os.environ['NEO4J_URI'] = getpass.getpass('NEO4J_URI:\\n')\n",
    "if not os.environ.get('NEO4J_USERNAME'):\n",
    "    os.environ['NEO4J_USERNAME'] = getpass.getpass('NEO4J_USERNAME:\\n')\n",
    "if not os.environ.get('NEO4J_PASSWORD'):\n",
    "    os.environ['NEO4J_PASSWORD'] = getpass.getpass('NEO4J_PASSWORD:\\n')\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171a832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[<Record count(n)=584>], summary=<neo4j._work.summary.ResultSummary object at 0x1292913d0>, keys=['count(n)'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# load into People nodes in Neo4j\n",
    "\n",
    "#instantiate driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "#test neo4j connection\n",
    "driver.execute_query(\"MATCH(n) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848beee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9799763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sangeethar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "nltk.download('punkt')\n",
    "encoding = get_encoding(\"cl100k_base\")\n",
    "\n",
    "# --- Step 1: Chunking function ---\n",
    "def chunk_text(text, max_tokens=400, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks, current_chunk, current_length = [], [], 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "        if current_length + sentence_tokens > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # add small overlap for context continuity\n",
    "            overlap_tokens = encoding.encode(\" \".join(current_chunk))[-overlap:]\n",
    "            overlap_text = encoding.decode(overlap_tokens)\n",
    "            current_chunk = [overlap_text, sentence]\n",
    "            current_length = len(encoding.encode(overlap_text)) + sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# This model produces 1536-dimensional vectors\n",
    "def embed_text(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# --- Step 3: Write transaction for chunks ---\n",
    "def create_chunk_node(tx, episode_number, file_name, order, text, embedding):\n",
    "    tx.run(\"\"\"\n",
    "    MERGE (ep:Episode {number: $episode_number})\n",
    "    MERGE (chunk:Chunk {order: $order, fileName: $file_name})\n",
    "      SET chunk.text = $text,\n",
    "          chunk.embedding = $embedding\n",
    "    MERGE (ep)-[:HAS_CHUNK]->(chunk)\n",
    "    MERGE (chunk)-[:BELONGS_TO_EPISODE]->(ep)\n",
    "    \"\"\", episode_number=episode_number, file_name=file_name, order=order, text=text, embedding=embedding)\n",
    "\n",
    "def add_chunks_to_neo4j(episode_number, file_name, text):\n",
    "    chunks = chunk_text(text)\n",
    "    with driver.session() as session:\n",
    "        for i, chunk in enumerate(chunks, start=1):\n",
    "            embedding = embed_text(chunk)\n",
    "            session.execute_write(\n",
    "                create_chunk_node,\n",
    "                episode_number,\n",
    "                file_name,\n",
    "                i,\n",
    "                chunk,\n",
    "                embedding\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c70398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "BASE_DIR = \"/Users/sangeethar/workspace/AI-Workspace/neo4j-employee-graph/neo4j-employee-graph/input-podcast-episodes-data/\"\n",
    "\n",
    "#Already Processed\n",
    "#osp.join(BASE_DIR, \"AnthropicAndModelContextProtocol-MCP-WithDavidSoriaParra-ep1836.txt\"),\n",
    "#osp.join(BASE_DIR, \"delayed-view-semantics-incremental-data-processing-ep473.txt\"),\n",
    "#osp.join(BASE_DIR, \"DuckLake-SimplifyingLakehouseEcosystem-ep480.txt\"),\n",
    "#osp.join(BASE_DIR, \"kuzudb-embeddable-graph-database-ep477.txt\"),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 4: Example usage ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bed22",
   "metadata": {},
   "source": [
    "#### Chunk and Embed PrompsAsFunctions-BAML and Iceberg-At_NetflixAndBeyong-1654.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATHS = [\n",
    "    osp.join(BASE_DIR, \"PromptsAsFunctions-BAML-Revolution_AI-Engineering-ep2025040307.txt\"),\n",
    "    osp.join(BASE_DIR, \"Iceberg-At-NetflixAndBeyond-RyanBlue-ep1654.txt\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150dd03",
   "metadata": {},
   "source": [
    "#### Chunk and Embed delayed-view-semantics-incremental-data-processing-ep473.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ae1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = osp.join(BASE_DIR, \"delayed-view-semantics-incremental-data-processing-ep473.txt\")\n",
    "file_name = \"delayed-view-semantics-incremental-data-processing-ep473.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "add_chunks_to_neo4j(473, file_name, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9b05b",
   "metadata": {},
   "source": [
    "#### Chunk and Embed Kuzu, MCP, DuckLate Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f785450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "\n",
    "BASE_DIR = \"/Users/sangeethar/workspace/AI-Workspace/neo4j-employee-graph/neo4j-employee-graph/input-podcast-episodes-data/\"\n",
    "\n",
    "FILE_PATHS = [\n",
    "    osp.join(BASE_DIR, \"AnthropicAndModelContextProtocol-MCP-WithDavidSoriaParra-ep1836.txt\"),\n",
    "    osp.join(BASE_DIR, \"DuckLake-SimplifyingLakehouseEcosystem-ep480.txt\"),\n",
    "    osp.join(BASE_DIR, \"kuzudb-embeddable-graph-database-ep477.txt\"),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24907432",
   "metadata": {},
   "source": [
    "#### Batch process the files from FILE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e978786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files in FILE_PATHS\n",
    "print(\"üöÄ Starting batch processing of all podcast episodes...\")\n",
    "\n",
    "for file_path in FILE_PATHS:\n",
    "    # Extract filename from full path\n",
    "    file_name = osp.basename(file_path)\n",
    "    \n",
    "    # Extract episode number from filename (assuming format: name-ep###.txt)\n",
    "    try:\n",
    "        episode_number = int(file_name.split('-ep')[-1].split('.')[0])\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not extract episode number from {file_name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing {file_name} (Episode #{episode_number})\")\n",
    "    \n",
    "    # Read file content\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Add chunks to Neo4j\n",
    "        add_chunks_to_neo4j(episode_number, file_name, text)\n",
    "        print(f\"‚úÖ Successfully processed {file_name}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\nüéâ Batch processing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d8b9d",
   "metadata": {},
   "source": [
    "#### Helper Methods to create embedding on Episode, Topic, Concept, Technology and ReferenceLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc17af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text summarization functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Text Summarization Functions for Embedding Generation\n",
    "\n",
    "def create_embedding_text_for_episode(episode_props):\n",
    "    \"\"\"Create a comprehensive text summary for Episode nodes\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Add basic episode information\n",
    "    if episode_props.get('name'):\n",
    "        text_parts.append(f\"Episode: {episode_props['name']}\")\n",
    "    \n",
    "    if episode_props.get('number'):\n",
    "        text_parts.append(f\"Episode Number: {episode_props['number']}\")\n",
    "    \n",
    "    if episode_props.get('description'):\n",
    "        text_parts.append(f\"Description: {episode_props['description']}\")\n",
    "    \n",
    "    if episode_props.get('published_date'):\n",
    "        text_parts.append(f\"Published: {episode_props['published_date']}\")\n",
    "    \n",
    "    if episode_props.get('link'):\n",
    "        text_parts.append(f\"Link: {episode_props['link']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def create_embedding_text_for_topic(topic_props):\n",
    "    \"\"\"Create a comprehensive text summary for Topic nodes\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if topic_props.get('name'):\n",
    "        text_parts.append(f\"Topic: {topic_props['name']}\")\n",
    "    \n",
    "    if topic_props.get('description'):\n",
    "        text_parts.append(f\"Description: {topic_props['description']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def create_embedding_text_for_concept(concept_props):\n",
    "    \"\"\"Create a comprehensive text summary for Concept nodes\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if concept_props.get('name'):\n",
    "        text_parts.append(f\"Concept: {concept_props['name']}\")\n",
    "    \n",
    "    if concept_props.get('description'):\n",
    "        text_parts.append(f\"Description: {concept_props['description']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def create_embedding_text_for_technology(tech_props):\n",
    "    \"\"\"Create a comprehensive text summary for Technology nodes\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if tech_props.get('name'):\n",
    "        text_parts.append(f\"Technology: {tech_props['name']}\")\n",
    "    \n",
    "    if tech_props.get('description'):\n",
    "        text_parts.append(f\"Description: {tech_props['description']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def create_embedding_text_for_reference_link(ref_props):\n",
    "    \"\"\"Create a comprehensive text summary for ReferenceLink nodes\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if ref_props.get('text'):\n",
    "        text_parts.append(f\"Reference: {ref_props['text']}\")\n",
    "    \n",
    "    if ref_props.get('url'):\n",
    "        text_parts.append(f\"URL: {ref_props['url']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "print(\"‚úÖ Text summarization functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c91879",
   "metadata": {},
   "source": [
    "#### Add Embedding properties to Episode, Topic, Concept, Technology, ReferenceLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Transaction Functions for Adding Embeddings\n",
    "\n",
    "def add_embedding_to_episode(tx, episode_number, embedding):\n",
    "    \"\"\"Add embedding property to an Episode node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (ep:Episode {number: $episode_number})\n",
    "    SET ep.embedding = $embedding\n",
    "    \"\"\", episode_number=episode_number, embedding=embedding)\n",
    "\n",
    "def add_embedding_to_topic(tx, topic_name, embedding):\n",
    "    \"\"\"Add embedding property to a Topic node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (t:Topic {name: $topic_name})\n",
    "    SET t.simple_embedding = $embedding\n",
    "    \"\"\", topic_name=topic_name, embedding=embedding)\n",
    "\n",
    "def add_embedding_to_concept(tx, concept_name, embedding):\n",
    "    \"\"\"Add embedding property to a Concept node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (c:Concept {name: $concept_name})\n",
    "    SET c.embedding = $embedding\n",
    "    \"\"\", concept_name=concept_name, embedding=embedding)\n",
    "\n",
    "def add_embedding_to_technology(tx, tech_name, embedding):\n",
    "    \"\"\"Add embedding property to a Technology node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (tech:Technology {name: $tech_name})\n",
    "    SET tech.embedding = $embedding\n",
    "    \"\"\", tech_name=tech_name, embedding=embedding)\n",
    "\n",
    "def add_embedding_to_reference_link(tx, ref_text, ref_url, embedding):\n",
    "    \"\"\"Add embedding property to a ReferenceLink node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (rl:ReferenceLink {text: $ref_text, url: $ref_url})\n",
    "    SET rl.embedding = $embedding\n",
    "    \"\"\", ref_text=ref_text, ref_url=ref_url, embedding=embedding)\n",
    "\n",
    "print(\"‚úÖ Neo4j transaction functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80f110",
   "metadata": {},
   "source": [
    "#### Add embedding to Episode, Topic, Concept, Technology, ReferenceLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab228a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Main Execution Function with Proper Transaction Handling\n",
    "# This fixes ResultConsumedError by collecting data INSIDE the transaction\n",
    "\n",
    "def add_embeddings_to_all_nodes():\n",
    "    \"\"\"Process all nodes and add embedding properties - FIXED for transaction scope\"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"üöÄ Starting embedding generation for all nodes...\")\n",
    "        \n",
    "        # Process Episode nodes\n",
    "        print(\"\\nüì∫ Processing Episode nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        episodes_data = session.execute_read(lambda tx: \n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (ep:Episode)\n",
    "                RETURN ep.number as number, properties(ep) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        episode_count = 0\n",
    "        # Process collected data AFTER transaction closes\n",
    "        for record in episodes_data:\n",
    "            episode_number = record['number']\n",
    "            episode_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_episode(episode_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_episode, episode_number, embedding)\n",
    "                    episode_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {episode_count} Episode nodes\")\n",
    "        \n",
    "        # Process Topic nodes\n",
    "        print(\"\\nüìö Processing Topic nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        topics_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (t:Topic)\n",
    "                RETURN t.name as name, properties(t) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        topic_count = 0\n",
    "        for record in topics_data:\n",
    "            topic_name = record['name']\n",
    "            topic_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_topic(topic_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_topic, topic_name, embedding)\n",
    "                    topic_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {topic_count} Topic nodes\")\n",
    "        \n",
    "        # Process Concept nodes\n",
    "        print(\"\\nüí° Processing Concept nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        concepts_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (c:Concept)\n",
    "                RETURN c.name as name, properties(c) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        concept_count = 0\n",
    "        for record in concepts_data:\n",
    "            concept_name = record['name']\n",
    "            concept_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_concept(concept_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_concept, concept_name, embedding)\n",
    "                    concept_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {concept_count} Concept nodes\")\n",
    "        \n",
    "        # Process Technology nodes\n",
    "        print(\"\\nüîß Processing Technology nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        techs_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (tech:Technology)\n",
    "                RETURN tech.name as name, properties(tech) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        tech_count = 0\n",
    "        for record in techs_data:\n",
    "            tech_name = record['name']\n",
    "            tech_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_technology(tech_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_technology, tech_name, embedding)\n",
    "                    tech_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {tech_count} Technology nodes\")\n",
    "        \n",
    "        # Process ReferenceLink nodes\n",
    "        print(\"\\nüîó Processing ReferenceLink nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        refs_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (rl:ReferenceLink)\n",
    "                RETURN rl.text as text, rl.url as url, properties(rl) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        ref_count = 0\n",
    "        for record in refs_data:\n",
    "            ref_text = record['text']\n",
    "            ref_url = record['url']\n",
    "            ref_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_reference_link(ref_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_reference_link, ref_text, ref_url, embedding)\n",
    "                    ref_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {ref_count} ReferenceLink nodes\")\n",
    "        \n",
    "        print(f\"\\nüéâ Embedding generation completed!\")\n",
    "        print(f\"Total nodes processed: {episode_count + topic_count + concept_count + tech_count + ref_count}\")\n",
    "\n",
    "print(\"‚úÖ FIXED embedding generation function loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d01ba9",
   "metadata": {},
   "source": [
    "#### Trigger Embedding creation for Episode, Topic, Concept, Technology, ReferenceLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the embedding generation\n",
    "print(\"üöÄ Starting embedding generation for all nodes...\")\n",
    "add_embeddings_to_all_nodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bb8a7",
   "metadata": {},
   "source": [
    "#### Verify embeddings were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9512b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check that embeddings were created\n",
    "print(\"üîç Verifying embeddings were created...\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Check Episode embeddings\n",
    "    episodes_with_embeddings = session.execute_read(lambda tx: tx.run(\"\"\"\n",
    "        MATCH (ep:Episode)\n",
    "        WHERE ep.embedding IS NOT NULL\n",
    "        RETURN count(ep) as count\n",
    "    \"\"\"))\n",
    "    print(f\"Episodes with embeddings: {episodes_with_embeddings.records[0]['count']}\")\n",
    "    \n",
    "    # Check Topic embeddings\n",
    "    topics_with_embeddings = session.execute_read(lambda tx: tx.run(\"\"\"\n",
    "        MATCH (t:Topic)\n",
    "        WHERE t.embedding IS NOT NULL\n",
    "        RETURN count(t) as count\n",
    "    \"\"\"))\n",
    "    print(f\"Topics with embeddings: {topics_with_embeddings.records[0]['count']}\")\n",
    "    \n",
    "    # Check Concept embeddings\n",
    "    concepts_with_embeddings = session.execute_read(lambda tx: tx.run(\"\"\"\n",
    "        MATCH (c:Concept)\n",
    "        WHERE c.embedding IS NOT NULL\n",
    "        RETURN count(c) as count\n",
    "    \"\"\"))\n",
    "    print(f\"Concepts with embeddings: {concepts_with_embeddings.records[0]['count']}\")\n",
    "    \n",
    "    # Check Technology embeddings\n",
    "    techs_with_embeddings = session.execute_read(lambda tx: tx.run(\"\"\"\n",
    "        MATCH (tech:Technology)\n",
    "        WHERE tech.embedding IS NOT NULL\n",
    "        RETURN count(tech) as count\n",
    "    \"\"\"))\n",
    "    print(f\"Technologies with embeddings: {techs_with_embeddings.records[0]['count']}\")\n",
    "    \n",
    "    # Check ReferenceLink embeddings\n",
    "    refs_with_embeddings = session.execute_read(lambda tx: tx.run(\"\"\"\n",
    "        MATCH (rl:ReferenceLink)\n",
    "        WHERE rl.embedding IS NOT NULL\n",
    "        RETURN count(rl) as count\n",
    "    \"\"\"))\n",
    "    print(f\"ReferenceLinks with embeddings: {refs_with_embeddings.records[0]['count']}\")\n",
    "\n",
    "print(\"‚úÖ Verification completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed2e0c",
   "metadata": {},
   "source": [
    "#### Add Comprehensive topic embedding (topic+concept+technologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05ee8511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comprehensive topic embedding functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Topic Data Collection and Enhanced Embedding Generation\n",
    "\n",
    "def get_comprehensive_topics_data():\n",
    "    \"\"\"Get all topics with their related concepts and technologies\"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"üîç Collecting comprehensive topic data...\")\n",
    "        \n",
    "        # Get comprehensive topic data with all related concepts and technologies\n",
    "        comprehensive_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (t:Topic)\n",
    "                OPTIONAL MATCH (t)-[:COVERS]->(c:Concept)\n",
    "                OPTIONAL MATCH (t)-[:COVERS]->(tech:Technology)\n",
    "                RETURN t.name as topic_name,\n",
    "                       collect(DISTINCT {\n",
    "                           concept_name: c.name,\n",
    "                           concept_description: c.description\n",
    "                       }) as concepts,\n",
    "                       collect(DISTINCT {\n",
    "                           tech_name: tech.name\n",
    "                       }) as technologies\n",
    "                ORDER BY t.name\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Collected data for {len(comprehensive_data)} topics\")\n",
    "        return comprehensive_data\n",
    "\n",
    "def create_comprehensive_embedding_text_for_topic(topic_data):\n",
    "    \"\"\"Create comprehensive embedding text for a topic including all related concepts and technologies\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Add topic information (only name, no description)\n",
    "    if topic_data.get('topic_name'):\n",
    "        text_parts.append(f\"Topic: {topic_data['topic_name']}\")\n",
    "    \n",
    "    # Add all related concepts\n",
    "    concepts = topic_data.get('concepts', [])\n",
    "    if concepts:\n",
    "        concept_texts = []\n",
    "        for concept in concepts:\n",
    "            if concept.get('concept_name'):\n",
    "                concept_text = f\"Concept: {concept['concept_name']}\"\n",
    "                if concept.get('concept_description'):\n",
    "                    concept_text += f\" - {concept['concept_description']}\"\n",
    "                concept_texts.append(concept_text)\n",
    "        \n",
    "        if concept_texts:\n",
    "            text_parts.append(\"Related Concepts: \" + \" | \".join(concept_texts))\n",
    "    \n",
    "    # Add all related technologies (only name, no description)\n",
    "    technologies = topic_data.get('technologies', [])\n",
    "    if technologies:\n",
    "        tech_texts = []\n",
    "        for tech in technologies:\n",
    "            if tech.get('tech_name'):\n",
    "                tech_text = f\"Technology: {tech['tech_name']}\"\n",
    "                tech_texts.append(tech_text)\n",
    "        \n",
    "        if tech_texts:\n",
    "            text_parts.append(\"Related Technologies: \" + \" | \".join(tech_texts))\n",
    "    \n",
    "    return \" | \".join(text_parts) if text_parts else None\n",
    "\n",
    "def add_comprehensive_embedding_to_topic(tx, topic_name, embedding):\n",
    "    \"\"\"Add comprehensive embedding property to a Topic node\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (t:Topic {name: $topic_name})\n",
    "    SET t.embedding = $embedding\n",
    "    \"\"\", topic_name=topic_name, embedding=embedding)\n",
    "\n",
    "def process_comprehensive_topic_embeddings():\n",
    "    \"\"\"Process all topics and create comprehensive embeddings including concepts and technologies\"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"üöÄ Starting comprehensive topic embedding generation...\")\n",
    "        \n",
    "        # Get comprehensive topic data\n",
    "        topics_data = get_comprehensive_topics_data()\n",
    "        \n",
    "        topic_count = 0\n",
    "        for topic_data in topics_data:\n",
    "            topic_name = topic_data['topic_name']\n",
    "            \n",
    "            # Create comprehensive embedding text\n",
    "            embedding_text = create_comprehensive_embedding_text_for_topic(topic_data)\n",
    "            if embedding_text:\n",
    "                # Generate embedding\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    # Add comprehensive embedding to topic\n",
    "                    session.execute_write(add_comprehensive_embedding_to_topic, topic_name, embedding)\n",
    "                    topic_count += 1\n",
    "                    print(f\"‚úÖ Processed comprehensive embedding for topic: {topic_name}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Processed {topic_count} topics with comprehensive embeddings!\")\n",
    "\n",
    "print(\"‚úÖ Comprehensive topic embedding functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2469b",
   "metadata": {},
   "source": [
    "#### Trigger Comprehensice Topic embedding creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a02e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive topic embedding generation...\n",
      "üöÄ Starting comprehensive topic embedding generation...\n",
      "üîç Collecting comprehensive topic data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: COVERS)} {position: line: 3, column: 38, offset: 70} for query: '\\n                MATCH (t:Topic)\\n                OPTIONAL MATCH (t)-[:COVERS]->(c:Concept)\\n                OPTIONAL MATCH (t)-[:COVERS]->(tech:Technology)\\n                RETURN t.name as topic_name,\\n                       collect(DISTINCT {\\n                           concept_name: c.name,\\n                           concept_description: c.description\\n                       }) as concepts,\\n                       collect(DISTINCT {\\n                           tech_name: tech.name\\n                       }) as technologies\\n                ORDER BY t.name\\n            '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: COVERS)} {position: line: 4, column: 38, offset: 128} for query: '\\n                MATCH (t:Topic)\\n                OPTIONAL MATCH (t)-[:COVERS]->(c:Concept)\\n                OPTIONAL MATCH (t)-[:COVERS]->(tech:Technology)\\n                RETURN t.name as topic_name,\\n                       collect(DISTINCT {\\n                           concept_name: c.name,\\n                           concept_description: c.description\\n                       }) as concepts,\\n                       collect(DISTINCT {\\n                           tech_name: tech.name\\n                       }) as technologies\\n                ORDER BY t.name\\n            '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collected data for 6 topics\n",
      "‚úÖ Processed comprehensive embedding for topic: Apache Iceberg Table Format and Data Lakes\n",
      "‚úÖ Processed comprehensive embedding for topic: BAML: Prompts as Structured Functions\n",
      "‚úÖ Processed comprehensive embedding for topic: Delayed View Semantics and Incremental Processing\n",
      "‚úÖ Processed comprehensive embedding for topic: Duck Lake: The SQL-Backed Open Standard for Simplified Lakehouse Metadata\n",
      "‚úÖ Processed comprehensive embedding for topic: Embeddable Graph Databases with KuzuDB\n",
      "‚úÖ Processed comprehensive embedding for topic: Model Context Protocol (MCP): The Open Standard for Context-Aware AI\n",
      "\n",
      "üéâ Processed 6 topics with comprehensive embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Execute comprehensive topic embedding generation\n",
    "print(\"üöÄ Starting comprehensive topic embedding generation...\")\n",
    "process_comprehensive_topic_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8b9d6",
   "metadata": {},
   "source": [
    "#### Verifying comprehensive topic embedding was creatted successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e9b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying comprehensive topic embeddings were created...\n",
      "Topics with comprehensive embeddings: 6\n",
      "\n",
      "üìä Sample comprehensive embeddings:\n",
      "  - Embeddable Graph Databases with KuzuDB: 1536 dimensions\n",
      "  - Duck Lake: The SQL-Backed Open Standard for Simplified Lakehouse Metadata: 1536 dimensions\n",
      "  - Delayed View Semantics and Incremental Processing: 1536 dimensions\n",
      "  - Model Context Protocol (MCP): The Open Standard for Context-Aware AI: 1536 dimensions\n",
      "  - Apache Iceberg Table Format and Data Lakes: 1536 dimensions\n",
      "  - BAML: Prompts as Structured Functions: 1536 dimensions\n",
      "‚úÖ Verification completed!\n"
     ]
    }
   ],
   "source": [
    "# Verification: Check comprehensive embeddings were created (FIXED for transaction scope)\n",
    "print(\"üîç Verifying comprehensive topic embeddings were created...\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Check topics with comprehensive embeddings\n",
    "    # FIX: Collect data INSIDE transaction using .data()\n",
    "    topics_data = session.execute_read(lambda tx:\n",
    "        [record.data() for record in tx.run(\"\"\"\n",
    "            MATCH (t:Topic)\n",
    "            WHERE t.comprehensive_embedding IS NOT NULL\n",
    "            RETURN count(t) as count\n",
    "        \"\"\")]\n",
    "    )\n",
    "    \n",
    "    # Process collected data AFTER transaction closes\n",
    "    for record in topics_data:\n",
    "        print(f\"Topics with comprehensive embeddings: {record['count']}\")\n",
    "        break  # Only need the first (and only) record\n",
    "    \n",
    "    # Show sample of comprehensive embedding data\n",
    "    # FIX: Collect data INSIDE transaction using .data()\n",
    "    sample_data = session.execute_read(lambda tx:\n",
    "        [record.data() for record in tx.run(\"\"\"\n",
    "            MATCH (t:Topic)\n",
    "            WHERE t.comprehensive_embedding IS NOT NULL\n",
    "            RETURN t.name as topic_name, \n",
    "                   size(t.comprehensive_embedding) as embedding_dimension\n",
    "            LIMIT 10\n",
    "        \"\"\")]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Sample comprehensive embeddings:\")\n",
    "    for record in sample_data:\n",
    "        print(f\"  - {record['topic_name']}: {record['embedding_dimension']} dimensions\")\n",
    "\n",
    "print(\"‚úÖ Verification completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ded17",
   "metadata": {},
   "source": [
    "#### List all the keys to verify the property is listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec630df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH (e:Topic)\n",
    "UNWIND keys(e) AS key // Get the list of keys for each episode and flatten them\n",
    "RETURN DISTINCT key AS topicPropertyKey\n",
    "ORDER BY key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH (e:Topic)\n",
    "RETURN properties(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05cb15",
   "metadata": {},
   "source": [
    "#### Handle Technology Nodes with missing names -- Process and create embeddings for them. These are auto detected Technology (entity) nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Technology Nodes with Missing Names\n",
    "\n",
    "def create_embedding_text_for_mentioned_technology(tech_props):\n",
    "    \"\"\"Create embedding text for Technology nodes that only have id property\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "  \n",
    "    if tech_props.get('name'):\n",
    "        text_parts.append(f\"Name: {tech_props['name']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts) if text_parts else None\n",
    "\n",
    "def process_technology_nodes_without_embeddings():\n",
    "    \"\"\"Process Technology nodes that have name and id but no embedding, add embedding\"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"üöÄ Processing Technology nodes with missing embeddings...\")\n",
    "        \n",
    "        # Get Technology nodes with name and id but no embedding\n",
    "        techs_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (n:Technology)\n",
    "                WHERE n.embedding IS NULL \n",
    "                  AND n.name IS NOT NULL\n",
    "                  AND n.id IS NOT NULL\n",
    "                RETURN n.name as name, properties(n) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        tech_count = 0\n",
    "        for record in techs_data:\n",
    "            tech_name = record['name']\n",
    "            tech_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_mentioned_technology(tech_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_embedding_to_technology, tech_name, embedding)\n",
    "                    tech_count += 1\n",
    "                    print(f\"‚úÖ Processed Technology: {tech_name}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Processed {tech_count} Technology nodes with missing embeddings!\")\n",
    "\n",
    "print(\"‚úÖ Technology name and embedding functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f284a6",
   "metadata": {},
   "source": [
    "#### Trigger Process Technology names without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Technology node processing\n",
    "print(\"üöÄ Starting processing of Technology nodes with missing embeddings...\")\n",
    "process_technology_nodes_without_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c990990",
   "metadata": {},
   "source": [
    "#### Embeddng generation summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229f38d",
   "metadata": {},
   "source": [
    "## Embedding Generation Summary\n",
    "\n",
    "### What was created:\n",
    "- **Episode embeddings**: Combines name, number, description, published_date, and link\n",
    "- **Topic embeddings**: Combines name and description\n",
    "- **Concept embeddings**: Combines name and description  \n",
    "- **Technology embeddings**: Combines name and description\n",
    "- **ReferenceLink embeddings**: Combines text and url\n",
    "\n",
    "### How to use the embeddings:\n",
    "\n",
    "1. **Similarity Search**: Use vector similarity to find related nodes\n",
    "2. **Semantic Search**: Search for nodes based on meaning rather than exact text\n",
    "3. **Recommendation Systems**: Find similar episodes, topics, or concepts\n",
    "4. **Clustering**: Group similar nodes together\n",
    "\n",
    "### Example Cypher queries with embeddings:\n",
    "\n",
    "```cypher\n",
    "// Find episodes similar to a given episode\n",
    "MATCH (ep1:Episode {number: 1654})\n",
    "MATCH (ep2:Episode)\n",
    "WHERE ep1 <> ep2 AND ep2.embedding IS NOT NULL\n",
    "RETURN ep2.name, ep2.number, \n",
    "       gds.similarity.cosine(ep1.embedding, ep2.embedding) as similarity\n",
    "ORDER BY similarity DESC\n",
    "LIMIT 5\n",
    "\n",
    "// Find topics similar to a concept\n",
    "MATCH (c:Concept {name: \"Data Engineering\"})\n",
    "MATCH (t:Topic)\n",
    "WHERE t.embedding IS NOT NULL\n",
    "RETURN t.name, \n",
    "       gds.similarity.cosine(c.embedding, t.embedding) as similarity\n",
    "ORDER BY similarity DESC\n",
    "LIMIT 3\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "- Embeddings are generated using OpenAI's `text-embedding-3-small` model\n",
    "- Each embedding is a 1536-dimensional vector\n",
    "- Embeddings are stored as arrays in Neo4j\n",
    "- Use GDS (Graph Data Science) library for similarity calculations\n",
    "- Consider updating embeddings when node properties change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b9f9b",
   "metadata": {},
   "source": [
    "#### Add Simple Embedding to Topic Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6b45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simple embedding functions for Topic nodes loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add Simple Embedding to Topic Nodes (Extracted from add_embeddings_to_all_nodes)\n",
    "\n",
    "def add_simple_embedding_to_topic_tx(tx, topic_name, embedding):\n",
    "    \"\"\"Add simple_embedding property to a Topic node (transaction function)\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "    MATCH (t:Topic {name: $topic_name})\n",
    "    SET t.simple_embedding = $embedding\n",
    "    \"\"\", topic_name=topic_name, embedding=embedding)\n",
    "\n",
    "def add_simple_embedding_to_topic():\n",
    "    \"\"\"\n",
    "    Process all Topic nodes and add simple_embedding properties.\n",
    "    This method extracts only the Topic-specific logic from add_embeddings_to_all_nodes().\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        print(\"üöÄ Starting simple embedding generation for Topic nodes...\")\n",
    "        \n",
    "        # Process Topic nodes\n",
    "        print(\"\\nüìö Processing Topic nodes...\")\n",
    "        # FIX: Collect data INSIDE transaction using .data()\n",
    "        topics_data = session.execute_read(lambda tx:\n",
    "            [record.data() for record in tx.run(\"\"\"\n",
    "                MATCH (t:Topic)\n",
    "                RETURN t.name as name, properties(t) as props\n",
    "            \"\"\")]\n",
    "        )\n",
    "        \n",
    "        topic_count = 0\n",
    "        for record in topics_data:\n",
    "            topic_name = record['name']\n",
    "            topic_props = dict(record['props'])\n",
    "            \n",
    "            # Create embedding text and generate embedding\n",
    "            embedding_text = create_embedding_text_for_topic(topic_props)\n",
    "            if embedding_text:\n",
    "                embedding = embed_text(embedding_text)\n",
    "                if embedding:\n",
    "                    session.execute_write(add_simple_embedding_to_topic_tx, topic_name, embedding)\n",
    "                    topic_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Processed {topic_count} Topic nodes\")\n",
    "        return topic_count\n",
    "\n",
    "print(\"‚úÖ Simple embedding functions for Topic nodes loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9c62",
   "metadata": {},
   "source": [
    "Trigger creation of simple_embedding property to Topic Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5787d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive topic embedding generation...\n",
      "üöÄ Starting simple embedding generation for Topic nodes...\n",
      "\n",
      "üìö Processing Topic nodes...\n",
      "‚úÖ Processed 6 Topic nodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üöÄ Starting comprehensive topic embedding generation...\")\n",
    "add_simple_embedding_to_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8177f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-employee-graph",
   "language": "python",
   "name": "neo4j-employee-graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
